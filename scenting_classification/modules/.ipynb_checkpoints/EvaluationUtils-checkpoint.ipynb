{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"ticks\")\n",
    "plt.rcParams[\"font.family\"] = \"Arial\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_for_ROC(loader, model, threshold=0.5):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        y_trues = []\n",
    "        y_preds = []\n",
    "        for i, (X, y) in enumerate(loader):\n",
    "            # Run batch through model \n",
    "            X = convert_X_for_resnet(X)\n",
    "            logits = model(X)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "\n",
    "            # Find thresholed predictions\n",
    "            thresholded_preds = probs[:,1] > threshold\n",
    "\n",
    "            y_trues.append(y.tolist())\n",
    "            y_preds.append(thresholded_preds.tolist())\n",
    "            \n",
    "    return y_trues, y_preds    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_dataframe(thresholds, loader, model):\n",
    "    thresholded_outputs = {f'threshold_{t:.02f}': {'true_ys': {}, 'predicted_ys': {}} for t in thresholds}\n",
    "\n",
    "    for i, (key, val) in enumerate(thresholded_outputs.items()):\n",
    "        print(f'\\r Threshold {key}')\n",
    "        \n",
    "        y_trues, y_preds = evaluate_for_ROC(loader, model, threshold=thresholds[i]) \n",
    "        y_trues = np.array(y_trues).flatten()\n",
    "        y_preds = np.array(y_preds).flatten()\n",
    "        thresholded_outputs[key]['true_ys'] = y_trues\n",
    "        thresholded_outputs[key]['predicted_ys'] = y_preds\n",
    "        \n",
    "    truth_outputs = pd.DataFrame(thresholded_outputs)\n",
    "    \n",
    "    # Compute TP, FP, TN, FN and add to df\n",
    "    for key, val in thresholded_outputs.items():\n",
    "        # Find TP, FP, TN, FN for each threshold\n",
    "        scenting_preds = truth_outputs[key]['predicted_ys'] == 1\n",
    "        scenting_trues = truth_outputs[key]['true_ys'] == 1\n",
    "        nonscenting_preds = truth_outputs[key]['predicted_ys'] == 0\n",
    "        nonscenting_trues = truth_outputs[key]['true_ys'] == 0\n",
    "\n",
    "        # Compute measurements\n",
    "        tp = sum(np.logical_and(scenting_preds, scenting_trues))\n",
    "        tn = sum(np.logical_and(nonscenting_preds, nonscenting_trues))\n",
    "        fp = sum(np.logical_and(scenting_preds, nonscenting_trues))\n",
    "        fn = sum(np.logical_and(nonscenting_preds, scenting_trues))\n",
    "\n",
    "        # Compute TPR and FPR\n",
    "        tpr = tp / (tp + fn)\n",
    "        fpr = fp / (fp + tn)\n",
    "\n",
    "        # Save all measurements\n",
    "        thresholded_outputs[key]['tp'] = tp\n",
    "        thresholded_outputs[key]['tn'] = tn\n",
    "        thresholded_outputs[key]['fp'] = fp\n",
    "        thresholded_outputs[key]['fn'] = fn\n",
    "        thresholded_outputs[key]['tpr'] = tpr\n",
    "        thresholded_outputs[key]['fpr'] = fpr\n",
    "        \n",
    "    truth_outputs = pd.DataFrame(thresholded_outputs)\n",
    "    return truth_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_AUC(fprs, tprs, loader):\n",
    "    \n",
    "    if loader == 'Test':\n",
    "        sorted_idxs = np.argsort(fprs)\n",
    "        sorted_fprs = np.array(fprs)[sorted_idxs]\n",
    "        sorted_tprs = np.array(tprs)[sorted_idxs]\n",
    "    else:\n",
    "        sorted_fprs = fprs\n",
    "        sorted_tprs = tprs\n",
    "    \n",
    "    auc = 0\n",
    "    for i in range(len(sorted_fprs)-1):\n",
    "        dx = np.abs(sorted_fprs[i+1] - sorted_fprs[i])\n",
    "        y_half = sorted_tprs[i] + np.abs(sorted_tprs[i+1] - sorted_tprs[i])/2\n",
    "        auc += y_half*dx\n",
    "        \n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ROC(truth_outputs, loader, color):\n",
    "    tprs = list(truth_outputs.loc['tpr'])\n",
    "    fprs = list(truth_outputs.loc['fpr'])\n",
    "    \n",
    "    auc = compute_AUC(fprs, tprs, loader)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(3, 3), dpi=150)\n",
    "\n",
    "    ax.plot(fprs, tprs, linewidth=2, c=color)\n",
    "    ax.plot([0, 1], [0, 1], '--', linewidth=2, c='pink')\n",
    "    ax.annotate(f'AUC: {auc:0.3f}', xy=(0.7,0.05), fontsize=10)\n",
    "\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.xaxis.grid(b=True, color='k', alpha=0.2, linestyle='--', linewidth=0.5)\n",
    "    ax.yaxis.grid(b=True, color=(0,0,0), alpha=0.2, linestyle='--', linewidth=0.5)\n",
    "    ax.set_xlabel('FPR')\n",
    "    ax.set_ylabel('TPR')\n",
    "    ax.set_title(f'{loader} set ROC');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fscore(tp, fn, fp):\n",
    "    test_recall = tp / (tp + fn)\n",
    "    test_precision = tp / (tp + fp)\n",
    "    test_f_score = (2*test_recall*test_precision) / (test_recall + test_precision)\n",
    "    return f'Test recall: {test_recall:0.2f}, Precision: {test_precision:0.2f}, F1 Score: {test_f_score:0.2f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(truth_outputs, loader, colormap):\n",
    "    test_cm = np.zeros((2,2), dtype=np.int)\n",
    "\n",
    "    tp = truth_outputs['threshold_0.50']['tp']\n",
    "    fp = truth_outputs['threshold_0.50']['fp']\n",
    "    tn = truth_outputs['threshold_0.50']['tn']\n",
    "    fn = truth_outputs['threshold_0.50']['fn']\n",
    "\n",
    "    test_cm[0,0] = tp\n",
    "    test_cm[0,1] = fp\n",
    "    test_cm[1,0] = fn\n",
    "    test_cm[1,1] = tn\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5,5), dpi=100)\n",
    "    sns.heatmap(test_cm, cmap=colormap, fmt=\"d\", annot=True, ax=ax, \n",
    "                xticklabels=['Scenting', 'Non-scenting'],\n",
    "                yticklabels=['Scenting', 'Non-Scenting'], )\n",
    "\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.xlabel('Actual')\n",
    "    plt.title(f'{loader} set confusion matrix');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
